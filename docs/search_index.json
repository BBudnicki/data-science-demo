[["index.html", "Data Science Demo Introduction View GitHub Code", " Data Science Demo Brandon Budnicki 2022-03-22 Introduction This R Bookdown website is my final project for the Introduction to Data Science Course ESS 580A7. I took this class in the Spring semester of 2022 as part of my candidacy in the Ecosystem Science &amp; Sustainability (ESS) PHD program at Colorado State University. View GitHub Code The code used to build this R Bookdown is hosted on GitHub. Data Science Demo GitHub "],["poudre-river-interactive-graph.html", "Chapter 1 Poudre River Interactive Graph 1.1 Methods 1.2 SiteDescription 1.3 Data Acquisition 1.4 Interactive Plot 1.5 Cameron Peak Fire", " Chapter 1 Poudre River Interactive Graph The Poudre river goes through northern Colorado through Fort Collins. By graphing the flow rate over time we can spot annual patterns &amp; sever weather events. Discharge data is downloaded by the dataRetrieval R package from the NWIS web service waterservices.usgs.gov 1.1 Methods The Poudre River at Lincoln Bridge is: Downstream of only a little bit of urban stormwater Near Odell Brewing CO Near an open space area and the Poudre River Trail Downstream of many agricultral diversions 1.2 SiteDescription 1.3 Data Acquisition q &lt;- readNWISdv( siteNumbers = &#39;06752260&#39;, parameterCd = &#39;00060&#39;, startDate = &#39;2017-01-01&#39;, endDate = &#39;2022-01-01&#39; ) %&gt;% rename(q = &#39;X_00060_00003&#39;) q_xts &lt;- xts(q$q, order.by = q$Date) 1.4 Interactive Plot series &lt;- cbind(points = q_xts) dygraph(series, main = &quot;Discharge in the Poudre River&quot;) %&gt;% dySeries(&quot;points&quot;, label = &quot;7 Day Averae&quot;, drawPoints = TRUE, pointSize = 3) %&gt;% dyAnnotation(&quot;2018-7-24&quot;, text = &quot;C&quot;, tooltip = &quot;Peak discharge&quot;) %&gt;% dyAnnotation(&quot;2018-7-22&quot;, text = &quot;B&quot;, tooltip = &quot;Larimer County lifts evacuation orders&quot;) %&gt;% dyAnnotation(&quot;2018-7-21&quot;, text = &quot;A&quot;, tooltip = &quot;Larimer County officials closed the Poudre River for all uses&quot;) %&gt;% dyRoller(rollPeriod = 1) %&gt;% dyRangeSelector(dateWindow = c(&quot;2018-07-01&quot;, &quot;2018-8-01&quot;)) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) Figure 1.1: Discharge in the Poudre River at the Lincoln Bride in Fort Collins, CO 1.5 Cameron Peak Fire Burn scars caused by the Cameron peak fire lead to dangerous discharge within the Poudre River on July 21st 2018 (Label A 1.1). Larimer County officials closed the Poudre River for all use and issued evacuation orders, finally reopening on July 22nd (Label B 1.1). These flood warnings were issued and then lifted before the peak discharge from this rainfall event on July 24th (Label C 1.1). "],["hayman-fire-google-earth-engine.html", "Chapter 2 Hayman Fire Google Earth Engine 2.1 NDVI over time 2.2 Correlation between NDVI &amp; NDMI 2.3 Snow and Vegitation 2.4 Greenest Month 2.5 Snowiest Month", " Chapter 2 Hayman Fire Google Earth Engine The Hayman Fire occurred June 8,2002 and was at the time the largest recorded wildfire in Colorado. By using Google Earth Engine to download remote sensing data, we can look at the impact the wildfire had on vegetation. Normalized Difference Vegetation Index (NDVI) Normalized Difference Snow Index (NDSI) Normalized Difference Moisture Index (NDMI) 2.1 NDVI over time There was a clear decrease in NDVI from pre to post burn at Site 2. Site 1 also saw a decrease, albeit quite a bit smaller. Figure 2.1: NDVI over time 2.2 Correlation between NDVI &amp; NDMI Exploring the correlation between NDVI &amp; NDMI looking at the summer months we can see that as surface moisture increases, vegetation increases. Site 2 saw a decrease in surface moisture and vegetation following the same ratio between NDVI and NDMI. Figure 2.2: Impact of surface moisture on vegatitation 2.3 Snow and Vegitation Looking at the previous years snow cover, we show little if any influence on vegetation growth the following summer. Both for the Site 1 &amp; Site 2 pre burn. There is a clear shift in vegetation due to the impacts of burned areas 2.3 from site 2 pre burn to site 2 post burn. The snow cover had lower maximums during this time. This could be due to darker surfaces but also could be due to outside factors such as warmer winters. Based on the sample size it is difficult to tell from this data. Figure 2.3: Impact of snow cover on vegatative growth 2.4 Greenest Month August is the greenest month based on the Normalized Difference Vegetation Index. This holds true for both unburned &amp; burned sites. Figure 2.4: Vegitation (NDVI) by Month 2.5 Snowiest Month Snow cover was greatest in January &amp; February. Site 1 had greater cover than site 2. This can be used as a proxy for snow fall, but is not a direct measurement as the freeze thaw cycle impacts the cover. Interestingly the snow cover post burn at site 2 was less than pre burn. This might be caused by a number of factors related or not related to the burn as theorized in 2.3. Figure 2.5: Average NDSI (snow cover) "],["simple-web-scraping-snow-studies.html", "Chapter 3 Simple web scraping Snow Studies 3.1 Scraping data 3.2 Data read-in 3.3 Plot snow data 3.4 Extract the meteorological data URLs. 3.5 Download the meteorological data. 3.6 Read data 3.7 Use the map function 3.8 Mean Air Temperture by Year 3.9 Air Temperture Error Codes 3.10 Mean Air Temperture Multiple Years 3.11 Average Daily Percipitation 3.12 Yearly Plots of Percepitation", " Chapter 3 Simple web scraping Snow Studies R can read html using either rvest, xml, or xml2 packages. Here we are going to navigate to the Center for Snow and Avalanche Studies Website and read a table in. This table contains links to data we want to programmatically download for three sites. We dont know much about these sites, but they contain incredibly rich snow, temperature, and precipitation data. 3.1 Scraping data Read the snow studies archive page to identify data for download. site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;24hr&#39;,.)] %&gt;% html_attr(&#39;href&#39;) 3.1.1 Download the data. #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes datapath = &#39;data/snow/&#39; dir.create(datapath) ## Warning in dir.create(datapath): &#39;data\\snow&#39; already exists file_names &lt;- paste0(datapath,dataset) for(i in 1:3){ download.file(links[i],destfile=file_names[i]) } downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(downloaded) 3.1.2 Download data in a map #Map version of the same for loop (downloading 3 files) if(evaluate == T){ map2(links[1:3],file_names[1:3],download.file) }else{print(&#39;data already downloaded&#39;)} 3.2 Data read-in #Pattern matching to only keep certain files snow_files &lt;- file_names %&gt;% .[!grepl(&#39;SG_24&#39;,.)] %&gt;% .[!grepl(&#39;PTSP&#39;,.)] our_snow_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_24hr.csv&#39;,&#39;&#39;,.) df &lt;- read_csv(file) %&gt;% select(Year,DOY,Sno_Height_M) %&gt;% mutate(site = name) } snow_data_full &lt;- map_dfr(snow_files,our_snow_reader) ## Rows: 6211 Columns: 52 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;,&quot; ## dbl (52): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 6575 Columns: 48 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;,&quot; ## dbl (48): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. 3.3 Plot snow data ## Year DOY Sno_Height_M site ## Min. :2003 Min. : 1.0 Min. :-3.523 Length:12786 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 Class :character ## Median :2012 Median :183.0 Median : 0.978 Mode :character ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 3.4 Extract the meteorological data URLs. Here we want you to use the rvest package to get the URLs for the SASP forcing and SBSP_forcing meteorological data sets. q1_links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;forcing&#39;,.)] %&gt;% html_attr(&#39;href&#39;) 3.5 Download the meteorological data. Use the download_file and str_split_fixed commands to download the data and save it in your data folder. You can use a for loop or a map function. q2_splits &lt;- str_split_fixed(q1_links,&#39;/&#39;,8) #Keep only the 8th column q2_dataset &lt;- q2_splits[,8] q2_file_names &lt;- paste0(datapath,q2_dataset) for(i in 1:2){ download.file(q1_links[i],destfile=q2_file_names[i]) } q2_downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(q2_downloaded) 3.6 Read data Write a custom function to read in the data and append a site column to the data. # this code grabs the variable names from the metadata pdf file q3_headers &lt;- pdf_text(&#39;https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf&#39;) %&gt;% readr::read_lines(.) %&gt;% trimws(.) %&gt;% str_split_fixed(.,&#39;\\\\.&#39;,2) %&gt;% .[,2] %&gt;% .[1:26] %&gt;% str_trim(side = &quot;left&quot;) q3_reader &lt;- function(file){ fileName = str_split_fixed(file,&#39;/&#39;,2)[,2] nameRight = str_split_fixed(fileName,&#39;_&#39;,2)[,2] nameLeft = str_split_fixed(nameRight,&#39;_&#39;,2)[,1] df &lt;- read.delim(file, header = FALSE, sep =&quot;&quot;,col.names = q3_headers,skip = 4) %&gt;% mutate(site = nameLeft) %&gt;% mutate(date = as.Date(paste(year, month, day, sep = &quot;-&quot;))) %&gt;% mutate(air_temp_k = air.temp..K.) %&gt;% mutate(air_temp_c = kelvin.to.celsius(air.temp..K.)) } 3.7 Use the map function Read in both meteorological files. Display a summary of your tibble. q4_full &lt;- map_dfr(q2_file_names,q3_reader) summary(q4_full[&#39;air_temp_k&#39;]) ## air_temp_k ## Min. :242.1 ## 1st Qu.:265.8 ## Median :272.6 ## Mean :272.6 ## 3rd Qu.:279.7 ## Max. :295.8 summary(q4_full[&#39;precip..kg.m.2.s.1.&#39;]) ## precip..kg.m.2.s.1. ## Min. :0.000e+00 ## 1st Qu.:0.000e+00 ## Median :0.000e+00 ## Mean :3.838e-05 ## 3rd Qu.:0.000e+00 ## Max. :6.111e-03 3.8 Mean Air Temperture by Year Make a line plot of mean temp by year by site (using the air temp [K] variable). Is there anything suspicious in the plot? Adjust your filtering if needed. Figure 3.1: Mean Air Temperture by Year Figure 3.1 shows the annual mean air temperature for both sites in a given year. 2003 &amp; 2004 have unusually low air temperatures. Figure 3.2: Boxplot Air Temperture by Year In figure 3.2 we see the air temperature in 2004 was only slightly lower, with the interquartile range being roughly the same. On the other hand 2003 had a significantly narrower interquartile range. At the scale of 1 year it is difficult to tell how much the 2003 data is skewed by collecting part of the year, if it was unusually cold that year, or if there was an issue with the instrumentation. Figure 3.3: Mean air temperture by month In 3.3 we see that it was unusually cold at both sites for the 2003 &amp; 2004 data. Much more for the SBSP. Due to only collecting data for the end of the year, 2003 data has a lower mean temperature as it does not include the summer months. Looking at the header file Serially-Complete-Metadata-text08.pdf we can see that they do include QC Code columns, but those values do not appear in the data set itself. The table of code values indicate some of the data has the followings errors, but the dates are not specified. 3.9 Air Temperture Error Codes 5001: missing data: use data from upper measurement location at same site (regression fill) 5003: missing data: use data from paired site (regression fill) 6000: before desired time period 6001: missing data: use data from upper measurement location at same site (regression fill) 6002: missing data: use data from paired site (regression fill) 6009: missing data: assume RH is 50%. 3.10 Mean Air Temperture Multiple Years Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Snow Angel Study Plot? Hint: https://ggplot2.tidyverse.org/reference/print.ggplot.html Figure 3.4: Line Chart Air Temperture by Month 2005-2010 Figure 3.5: Line Chart Air Temperture by Month 2005-2010 Figure 3.6: Line Chart Air Temperture by Month 2005-2010 Figure 3.7: Line Chart Air Temperture by Month 2005-2010 Figure 3.8: Line Chart Air Temperture by Month 2005-2010 Figure 3.9: Line Chart Air Temperture by Month 2005-2010 Monthly average temperatures at the Snow Angel Study Plot are consistently warmer than the Senator Beck Study Plot. At no point from 2005 to 2010 was the Snow Angel Study Plot mean monthly temperature lower than the Senator Beck Study Plot. Figure 3.10: Line Chart Air Temperture by Month Figure 3.11: Line Chart Air Temperture by Month 3.11 Average Daily Percipitation Make a plot of average daily precipitation by day of year (averaged across all available years). Color each site. Figure 3.12: Average daily percipitation The number of years is not great enough to handle daily precipitation well. Trying monthly. Figure 3.13: Average daily percipitation in a given month That was not helpful for finding a pattern. Also note: the daily precipitation is identical at both sites. 3.12 Yearly Plots of Percepitation Use a function and for loop to create yearly plots of precipitation by day of year. Color each site. Figure 3.14: Percipitation By Month "],["lagos-spatial-analysis.html", "Chapter 4 LAGOS Spatial Analysis 4.1 Loading in data 4.2 Map of Iowa &amp; Illinois 4.3 Subset LAGOS data 4.4 Distribution of lake size in Iowa vs. Minnesota 4.5 Lakes in Iowa &amp; Illinois by lake area 4.6 Future investigation", " Chapter 4 LAGOS Spatial Analysis lagoslakes.org collects data for studying lakes through time. Here we visualize lakes in Minnesota, Iowa, &amp; Illinois geospatially using Leaflet. 4.1 Loading in data 4.1.1 First download and then specifically grab the locus (or site lat longs) #Lagos download script #LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()) #Load in lagos lagos &lt;- lagosne_load() ## Warning in (function (version = NULL, fpath = NA) : LAGOSNE version unspecified, ## loading version: 1.087.3 #Grab the lake centroid info lake_centers &lt;- lagos$locus 4.1.2 Convert to spatial data #Look at the column names #names(lake_centers) #Look at the structure #str(lake_centers) #View the full dataset #View(lake_centers %&gt;% slice(1:100)) spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) %&gt;% st_transform(2163) #Subset for plotting subset_spatial &lt;- spatial_lakes %&gt;% slice(1:100) subset_baser &lt;- spatial_lakes[1:100,] #Dynamic mapviewer #mapview(subset_spatial, layer.name=&quot;Lakes&quot;) 4.1.3 Subset to only Minnesota states &lt;- us_states() #Plot all the states to check if they loaded #mapview(states) minnesota &lt;- states %&gt;% filter(name == &#39;Minnesota&#39;) %&gt;% st_transform(2163) #Subset lakes based on spatial position minnesota_lakes &lt;- spatial_lakes[minnesota,] #Plotting the first 1000 lakes minnesota_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;, layer.name=&quot;Lake Area (HA)&quot;) + mapview(minnesota) 4.2 Map of Iowa &amp; Illinois #Plot all the states to check if they loaded #mapview(states) #polygon for Iowa iowa &lt;- states %&gt;% filter(name == &#39;Iowa&#39;) %&gt;% st_transform(2163) #Ploygon for Illionois illinois &lt;- states %&gt;% filter(name == &#39;Illinois&#39;) %&gt;% st_transform(2163) #Digplay combined map mapview(illinois, alpha.regions = 0.4, aplha = 1, col.regions = &quot;yellow&quot;) + mapview(iowa, alpha.regions = 0.4, aplha = 1, col.regions = &quot;red&quot;) #Filter lakes based on state iowa_lakes &lt;- spatial_lakes[iowa,] illinois_lakes &lt;- spatial_lakes[illinois,] 4.3 Subset LAGOS data Subset LAGOS data to these sites, how many sites are in Illinois and Iowa combined? How does this compare to Minnesota? # Create bar chart of lakes size distribution minnesota_length = length(minnesota_lakes$lagoslakeid) iowa_illinois_lakes = rbind(illinois_lakes, iowa_lakes) iowa_illinois_length = length(iowa_illinois_lakes$lagoslakeid) Minnesota has 29038 lakes while Iowa &amp; Illinois combined have 16466 lakes 4.4 Distribution of lake size in Iowa vs. Minnesota Minnesota has bigger lakes than Iowa. Both are skewed to the right. Figure 4.1: Distribution of lake size in Iowa vs. Minnesota 4.5 Lakes in Iowa &amp; Illinois by lake area # Arrange lakes so larger ones are on top iowa_illinois_map = iowa_illinois_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;, layer.name=&quot;Lake Area (HA)&quot;, canvas = TRUE, at=c(0,10,100,1000,10000)) # Combine map layersfor display mapview(illinois, canvas = TRUE , alpha.regions = 0.4, aplha = 1, col.regions = &quot;yellow&quot;) + mapview(iowa, canvas = TRUE , alpha.regions = 0.4, aplha = 1, col.regions = &quot;red&quot;) + iowa_illinois_map 4.6 Future investigation What other data sources might we use to understand how reservoirs and natural lakes vary in size in these three states? Ground water &amp; especially water table depth data would be helpful for looking at how the lakes interact with one another. Long term weather &amp; climate (rainfall, snowfall, and Total Solar Radiance) would be helpful in to for understanding water origin &amp; evaporation. "],["lagosne-water-quality-analysis.html", "Chapter 5 LAGOSNE Water Quality Analysis 5.1 Loading in data 5.2 Mean Chlorophyll A map 5.3 Correlation between Secchi Disk Depth and Chlorophyll A 5.4 Which states have the most data? 5.5 Why 200 observations 5.6 25 observations per lake", " Chapter 5 LAGOSNE Water Quality Analysis lagoslakes.org archives observations of lakes over time. Here we visualize lakes in Minnesota, Iowa, &amp; Illinois geospatially using Leaflet. In this analysis we will look at Chlorophyll A &amp; Secchi depth (meters). Chlorophyll A is a proxy measure for the amount of algea in a water body. Secchi depth is a measure of water clarity. 5.1 Loading in data 5.1.1 Download and identify the site locus(latiutde longitude) #Lagos download script #lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus # Make an sf object spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) #Grab the water quality data nutr &lt;- lagos$epi_nutr #Look at column names #names(nutr) # subset columns nutr to only keep key info that we want clarity_only &lt;- nutr %&gt;% dplyr::select(lagoslakeid,sampledate,chla,doc,secchi) %&gt;% mutate(sampledate = as.character(sampledate) %&gt;% ymd(.)) 5.1.2 Filter sites with at least 200 observations This is done to ensure we have enough data for a given lake to conduct analysis. This was arbitrarily chosen. See Why 200 observations for an analysis of the distribution of observations per lake. #Look at the number of rows of data #nrow(clarity_only) chla_secchi &lt;- clarity_only %&gt;% filter(!is.na(chla), !is.na(secchi)) # How many observatiosn did we lose? filteredObservations = nrow(clarity_only) - nrow(chla_secchi) # Keep only the lakes with at least 200 observations of secchi and chla chla_secchi_200 &lt;- chla_secchi %&gt;% group_by(lagoslakeid) %&gt;% mutate(count = n()) %&gt;% filter(count &gt; 200) # Join water quality data to spatial data spatial_200 &lt;- inner_join( spatial_lakes,chla_secchi_200 %&gt;% distinct(lagoslakeid,.keep_all=T), by=&#39;lagoslakeid&#39; ) We lost 651095 observations because they were missing Secchi or Chlorophyll data. 5.2 Mean Chlorophyll A map ### Take the mean Chlorophyll A and Secchi by lake mean_values_200 &lt;- chla_secchi_200 %&gt;% # Take summary by lake id group_by(lagoslakeid) %&gt;% # take mean chl_a per lake id summarize( mean_chla = mean(chla,na.rm=T), mean_secchi=mean(secchi,na.rm=T) ) %&gt;% #Get rid of NAs filter( !is.na(mean_chla), !is.na(mean_secchi) ) %&gt;% # Take the log base 10 of the mean_chl mutate(log10_mean_chla = log10(mean_chla)) #Join datasets mean_spatial &lt;- inner_join( spatial_lakes, mean_values_200, by=&#39;lagoslakeid&#39; ) #Make a map mapview(mean_spatial,zcol=&#39;log10_mean_chla&#39;, layer.name = &#39;Mean Chlorophyll A Log 10&#39;) 5.3 Correlation between Secchi Disk Depth and Chlorophyll A Chlorophyll blocks light and obscures the secchi disk. As chlorophyll increases, Secchi depth decreases. Additionally algae are the primary producers of chlorophyll along with weeds. Lakes that have a higher nutrient load leads to more chlorophyll, these dissolved nutrients also tend to cloud waters and obscure secchi disks. Finally, deeper lakes tend to produce less photosynthetic algae due to increased mechanical mixing of different water layers. Figure 5.1: Secchi depth vs Chlorophyll 5.4 Which states have the most data? # Make a lagos spatial dataset that has the total number of counts per site. # Get count for each lake lago_summary = chla_secchi %&gt;% #slice(1:10000) %&gt;% group_by(lagoslakeid) %&gt;% summarize( mean_chla = mean(chla,na.rm=T), mean_secchi=mean(secchi,na.rm=T), count=n() ) ## Join to lake location lago_location_summary = merge( x = lago_summary, y = lake_centers, by = &quot;lagoslakeid&quot;, all.x = TRUE ) %&gt;% st_as_sf(coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;),crs=4326) # Show all points on the map #mapview(lago_location_summary) # join this point dataset to the us_boundaries data. state_bounds = us_states() %&gt;% dplyr::select(state_name,state_abbr) lago_location_summary_join_state = st_join( x = lago_location_summary, y = state_bounds, left = TRUE ) lago_location_summary_join_state_200 = lago_location_summary_join_state %&gt;% filter(count &gt; 200) # Group by state and sum all the observations in that state and arrange that data from most to least total observations per state. state_data = lago_location_summary_join_state %&gt;% group_by(state_name) %&gt;% summarize( mean_chla = mean(mean_chla,na.rm=T), mean_secchi=mean(mean_secchi,na.rm=T), count=sum(count) ) %&gt;% arrange(desc(count)) # verify all observations totaled correctly **success** #sum(state_data$count) state_data_200 = lago_location_summary_join_state_200 %&gt;% group_by(state_name) %&gt;% summarize( mean_chla = mean(mean_chla,na.rm=T), mean_secchi=mean(mean_secchi,na.rm=T), count=sum(count) ) %&gt;% arrange(desc(count)) # map of where state with most values are state_obs_count = st_join( x = state_bounds, y = state_data, left = TRUE ) # map of where state with most values are state_obs_count_200 = st_join( x = state_bounds, y = state_data_200, left = TRUE ) 5.4.1 Number of observations for all lakes 5.4.2 Number of observations for lakes with more than 200 observations 5.4.3 Spatial pattern in Secchi disk depth for lakes with at least 200 observations The lakes with more than 200 observations are all centered on urban areas. This show more of a bias towards the accessibility rather than a spatial connection with secchi disk depths. 5.5 Why 200 observations Histogram 5.2 shows the distribution of observations / lake. The 200 observations per lake is somewhat arbitrary. Looking at 10 to 50 observations per lake would capture more than the outlier lakes with an extreme amount of observation activity. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 5.2: Distribution of observations / lake 5.6 25 observations per lake At 25 observations per lake this demonstrates 2 clear spatial relations for Secchi depth: Increasing latitude is positively correlated with Secchi depth. This could be to decreased agricultural activity in the north. Increased distance from population centers is positively correclated with Secchi depth. This could be to less human made pollution adding nutrient to the water. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
